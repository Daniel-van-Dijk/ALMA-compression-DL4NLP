Large language models have significantly advanced machine translation, but their growing size and computational demands make them challenging to deploy efficiently. In this project, we focus on compressing the ALMA-7B model, aiming to reduce model size and computational costs using quantization, pruning, and knowledge distillation, while maintaining high translation quality.


%Our goal is to explore various pruning techniques, including magnitude pruning, Wanda pruning, and GBLM-pruning, to make the model more sparse and efficient. We will also use QLoRA (Quantized Low-Rank Adaptation) for fine-tuning and knowledge distillation to transfer knowledge from the larger model to a smaller, more efficient one. Performance will be measured using BLEU scores and COMET scores.